{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('InsightFace_Pytorch/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for root, dirs, files in os.walk(\"../dataset/CelebAMask-HQ/CelebA-HQ-img/\"):\n",
    "    for name in files:\n",
    "        if name.endswith('jpg') or name.endswith('png'):\n",
    "            p = os.path.join(root, name)\n",
    "            filenames.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = filenames[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(p)[:, :, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = mtcnn.align_multi(Image.fromarray(img), min_face_size=64, crop_size=(256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Backbone(50, 0.6, 'ir_se')#.to(device)\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load('model_ir_se50.pth', map_location='cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = trans.Compose([\n",
    "    trans.ToTensor(),\n",
    "    trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for face in faces:\n",
    "    scaled_img = face.resize((112, 112), Image.ANTIALIAS)\n",
    "    with torch.no_grad():\n",
    "        embed = model(test_transform(scaled_img).unsqueeze(0)).squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoaderimport sys\n",
    "\n",
    "sys.path.append('InsightFace_Pytorch/')\n",
    "\n",
    "import torch\n",
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import PIL.Image as Image\n",
    "from model import Backbone, Arcface, MobileFaceNet, Am_softmax, l2_norm\n",
    "from torchvision import transforms as trans\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import random\n",
    "\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FaceDatasetNoPrep(TensorDataset):\n",
    "    def __init__(self, image_folder=image_folder, same_prob=same_prob):\n",
    "        self.same_prob = same_prob\n",
    "        \n",
    "        self.filenames = []\n",
    "        for root, dirs, files in os.walk(image_folder):\n",
    "            for name in files:\n",
    "                if name.endswith('jpg') or name.endswith('png'):\n",
    "                    p = os.path.join(root, name)\n",
    "                    self.filenames.append(p)\n",
    "        \n",
    "#         self.mtcnn = MTCNN()\n",
    "        \n",
    "        \n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ColorJitter(0.2, 0.2, 0.2, 0.01),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        X_s = cv2.imread(self.filenames[item])[:, :, ::-1]\n",
    "        if random.random() > self.same_prob:\n",
    "            random_target_img_idx = random.randint(0, len(self.filenames)-1)\n",
    "            image_path = self.filenames[random_target_img_idx]\n",
    "            Xt = cv2.imread(image_path)[:, :, ::-1]\n",
    "            \n",
    "            same_person = 0\n",
    "            \n",
    "            return self.transforms(X_s), self.transforms(X_t), same_person\n",
    "        \n",
    "        else:\n",
    "            Xt = X_s.copy()\n",
    "            same_person = 1\n",
    "            \n",
    "            return self.transforms(X_s), self.transforms(X_t), same_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"dataset/CelebAMask-HQ/CelebA-HQ-img/\"\n",
    "same_prob = 0.2\n",
    "\n",
    "class FaceDataset(TensorDataset):\n",
    "    def __init__(self, image_folder=image_folder, same_prob=same_prob):\n",
    "        self.same_prob = same_prob\n",
    "        \n",
    "        self.filenames = []\n",
    "        for root, dirs, files in os.walk(image_folder):\n",
    "            for name in files:\n",
    "                if name.endswith('jpg') or name.endswith('png'):\n",
    "                    p = os.path.join(root, name)\n",
    "                    self.filenames.append(p)\n",
    "        \n",
    "        self.mtcnn = MTCNN()\n",
    "        \n",
    "        \n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ColorJitter(0.2, 0.2, 0.2, 0.01),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    def __getitem__(self, item):\n",
    "        img = cv2.imread(self.filenames[item])[:, :, ::-1]\n",
    "        print('get')\n",
    "        if random.random() > self.same_prob:\n",
    "\n",
    "            random_target_img_idx = random.randint(0, len(self.filenames)-1)\n",
    "            image_path = self.filenames[random_target_img_idx]\n",
    "            Xt = cv2.imread(image_path)[:, :, ::-1]\n",
    "\n",
    "            same_person = 0\n",
    "            X_s = self.align_and_crop(img)\n",
    "            X_t = self.align_and_crop(Xt)\n",
    "            \n",
    "            if not X_s:\n",
    "                item += 1\n",
    "                return self.__getitem__(item)\n",
    "            if not X_t:\n",
    "                return self.__getitem__(item)\n",
    "            return self.transforms(X_s), self.transforms(X_t), same_person\n",
    "        else:\n",
    "            Xt = img.copy()\n",
    "            same_person = 1\n",
    "\n",
    "            X_s = self.align_and_crop(img)\n",
    "            X_t = self.align_and_crop(Xt)\n",
    "            if not X_s:\n",
    "                item += 1\n",
    "                return self.__getitem__(item)\n",
    "            if not X_t:\n",
    "                return self.__getitem__(item)\n",
    "            return self.transforms(X_s), self.transforms(X_t), same_person\n",
    "        \n",
    "        \n",
    "    def align_and_crop(self, img):\n",
    "        faces = self.mtcnn.align_multi(Image.fromarray(img), min_face_size=64, crop_size=(256, 256))\n",
    "        if not faces:\n",
    "            return False\n",
    "\n",
    "        return faces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = \"dataset/CelebAMask-HQ/CelebA-HQ-img/\"\n",
    "target_folder = \"dataset/celeb_cropped\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = FaceDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(image_folder):\n",
    "    for name in files:\n",
    "        if name.endswith('jpg') or name.endswith('png'):\n",
    "            p = os.path.join(root, name)\n",
    "            img = cv2.imread(p)[:, :, ::-1]\n",
    "            result = dl.align_and_crop(img)\n",
    "            if not result:\n",
    "                continue\n",
    "            result.save(os.path.join(target_folder, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL = DataLoader(dl, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in DL:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s, X_t, same = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s.shape, X_t.shape, same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataLoader import FaceDatasetNoPrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FaceDatasetNoPrep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29539"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 3, 256, 256]) tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "for data in dl:\n",
    "    break\n",
    "    \n",
    "print(data[0].shape, data[1].shape, data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = os.listdir('dataset/celeb_cropped/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepathes = [os.path.join('dataset/celeb_cropped', image) for image in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29613"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filepathes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = np.random.choice(images, size = int(len(images) * 0.03) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "888"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for image in valid:\n",
    "    shutil.move(os.path.join('dataset/celeb_cropped', image), os.path.join('dataset/celeb_cropped_valid', image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faceswap",
   "language": "python",
   "name": "faceswap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
